{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "\n",
    "\t# Layer initialization\n",
    "\tdef __init__(self, n_inputs, n_neurons,\n",
    "\t\t\t\t weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "\t\t\t\t bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "\t\t# Initialize weights and biases\n",
    "\t\tself.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "\t\tself.biases = np.zeros((1, n_neurons))\n",
    "\t\t# Set regularization strength\n",
    "\t\tself.weight_regularizer_l1 = weight_regularizer_l1\n",
    "\t\tself.weight_regularizer_l2 = weight_regularizer_l2\n",
    "\t\tself.bias_regularizer_l1 = bias_regularizer_l1\n",
    "\t\tself.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "\t# Forward pass\n",
    "\tdef forward(self, inputs, training):\n",
    "\t\t# Remember input values\n",
    "\t\tself.inputs = inputs\n",
    "\t\t# Calculate output values from inputs, weights and biases\n",
    "\t\tself.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\t# Backward pass\n",
    "\tdef backward(self, dvalues):\n",
    "\t\t# Gradients on parameters\n",
    "\t\tself.dweights = np.dot(self.inputs.T, dvalues)\n",
    "\t\tself.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "\t\t# Gradients on regularization\n",
    "\t\t# L1 on weights\n",
    "\t\tif self.weight_regularizer_l1 > 0:\n",
    "\t\t\tdL1 = np.ones_like(self.weights)\n",
    "\t\t\tdL1[self.weights < 0] = -1\n",
    "\t\t\tself.dweights += self.weight_regularizer_l1 * dL1\n",
    "\t\t# L2 on weights\n",
    "\t\tif self.weight_regularizer_l2 > 0:\n",
    "\t\t\tself.dweights += 2 * self.weight_regularizer_l2 * \\\n",
    "\t\t\t\t\t\t\t self.weights\n",
    "\t\t# L1 on biases\n",
    "\t\tif self.bias_regularizer_l1 > 0:\n",
    "\t\t\tdL1 = np.ones_like(self.biases)\n",
    "\t\t\tdL1[self.biases < 0] = -1\n",
    "\t\t\tself.dbiases += self.bias_regularizer_l1 * dL1\n",
    "\t\t# L2 on biases\n",
    "\t\tif self.bias_regularizer_l2 > 0:\n",
    "\t\t\tself.dbiases += 2 * self.bias_regularizer_l2 * \\\n",
    "\t\t\t\t\t\t\tself.biases\n",
    "\n",
    "\t\t# Gradient on values\n",
    "\t\tself.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\t# Retrieve layer parameters\n",
    "\tdef get_parameters(self):\n",
    "\t\treturn self.weights, self.biases\n",
    "\n",
    "\t# Set weights and biases in a layer instance\n",
    "\tdef set_parameters(self, weights, biases):\n",
    "\t\tself.weights = weights\n",
    "\t\tself.biases = biases\n",
    "\n",
    "\n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "\t# Forward pass\n",
    "\tdef forward(self, inputs, training):\n",
    "\t\t# Remember input values\n",
    "\t\tself.inputs = inputs\n",
    "\t\t# Calculate output values from inputs\n",
    "\t\tself.output = np.maximum(0, inputs)\n",
    "\n",
    "\t# Backward pass\n",
    "\tdef backward(self, dvalues):\n",
    "\t\t# Since we need to modify original variable,\n",
    "\t\t# let's make a copy of values first\n",
    "\t\tself.dinputs = dvalues.copy()\n",
    "\n",
    "\t\t# Zero gradient where input values were negative\n",
    "\t\tself.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\t# Calculate predictions for outputs\n",
    "\tdef predictions(self, outputs):\n",
    "\t\treturn outputs\n",
    "\n",
    "\n",
    "\n",
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "\t# Forward pass\n",
    "\tdef forward(self, inputs, training):\n",
    "\t\t# Remember input values\n",
    "\t\tself.inputs = inputs\n",
    "\n",
    "\t\t# Get unnormalized probabilities\n",
    "\t\texp_values = np.exp(inputs - np.max(inputs, axis=1,\n",
    "\t\t\t\t\t\t\t\t\t\t\tkeepdims=True))\n",
    "\t\t# Normalize them for each sample\n",
    "\t\tprobabilities = exp_values / np.sum(exp_values, axis=1,\n",
    "\t\t\t\t\t\t\t\t\t\t\tkeepdims=True)\n",
    "\n",
    "\t\tself.output = probabilities\n",
    "\n",
    "\t# Backward pass\n",
    "\tdef backward(self, dvalues):\n",
    "\n",
    "\t\t# Create uninitialized array\n",
    "\t\tself.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "\t\t# Enumerate outputs and gradients\n",
    "\t\tfor index, (single_output, single_dvalues) in \\\n",
    "\t\t\t\tenumerate(zip(self.output, dvalues)):\n",
    "\t\t\t# Flatten output array\n",
    "\t\t\tsingle_output = single_output.reshape(-1, 1)\n",
    "\t\t\t# Calculate Jacobian matrix of the output\n",
    "\t\t\tjacobian_matrix = np.diagflat(single_output) - \\\n",
    "\t\t\t\t\t\t\t  np.dot(single_output, single_output.T)\n",
    "\t\t\t# Calculate sample-wise gradient\n",
    "\t\t\t# and add it to the array of sample gradients\n",
    "\t\t\tself.dinputs[index] = np.dot(jacobian_matrix,\n",
    "\t\t\t\t\t\t\t\t\t\t single_dvalues)\n",
    "\n",
    "\t# Calculate predictions for outputs\n",
    "\tdef predictions(self, outputs):\n",
    "\t\treturn np.argmax(outputs, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Adam optimizer\n",
    "class Optimizer_Adam:\n",
    "\n",
    "\t# Initialize optimizer - set settings\n",
    "\tdef __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7,\n",
    "\t\t\t\t beta_1=0.9, beta_2=0.999):\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.current_learning_rate = learning_rate\n",
    "\t\tself.decay = decay\n",
    "\t\tself.iterations = 0\n",
    "\t\tself.epsilon = epsilon\n",
    "\t\tself.beta_1 = beta_1\n",
    "\t\tself.beta_2 = beta_2\n",
    "\n",
    "\t# Call once before any parameter updates\n",
    "\tdef pre_update_params(self):\n",
    "\t\tif self.decay:\n",
    "\t\t\tself.current_learning_rate = self.learning_rate * \\\n",
    "\t\t\t\t(1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "\t# Update parameters\n",
    "\tdef update_params(self, layer):\n",
    "\n",
    "\t\t# If layer does not contain cache arrays,\n",
    "\t\t# create them filled with zeros\n",
    "\t\tif not hasattr(layer, 'weight_cache'):\n",
    "\t\t\tlayer.weight_momentums = np.zeros_like(layer.weights)\n",
    "\t\t\tlayer.weight_cache = np.zeros_like(layer.weights)\n",
    "\t\t\tlayer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\t\t\tlayer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "\n",
    "\t\t# Update momentum  with current gradients\n",
    "\t\tlayer.weight_momentums = self.beta_1 * \\\n",
    "\t\t\t\t\t\t\t\t layer.weight_momentums + \\\n",
    "\t\t\t\t\t\t\t\t (1 - self.beta_1) * layer.dweights\n",
    "\t\tlayer.bias_momentums = self.beta_1 * \\\n",
    "\t\t\t\t\t\t\t   layer.bias_momentums + \\\n",
    "\t\t\t\t\t\t\t   (1 - self.beta_1) * layer.dbiases\n",
    "\t\t# Get corrected momentum\n",
    "\t\t# self.iteration is 0 at first pass\n",
    "\t\t# and we need to start with 1 here\n",
    "\t\tweight_momentums_corrected = layer.weight_momentums / \\\n",
    "\t\t\t(1 - self.beta_1 ** (self.iterations + 1))\n",
    "\t\tbias_momentums_corrected = layer.bias_momentums / \\\n",
    "\t\t\t(1 - self.beta_1 ** (self.iterations + 1))\n",
    "\t\t# Update cache with squared current gradients\n",
    "\t\tlayer.weight_cache = self.beta_2 * layer.weight_cache + \\\n",
    "\t\t\t(1 - self.beta_2) * layer.dweights**2\n",
    "\t\tlayer.bias_cache = self.beta_2 * layer.bias_cache + \\\n",
    "\t\t\t(1 - self.beta_2) * layer.dbiases**2\n",
    "\t\t# Get corrected cache\n",
    "\t\tweight_cache_corrected = layer.weight_cache / \\\n",
    "\t\t\t(1 - self.beta_2 ** (self.iterations + 1))\n",
    "\t\tbias_cache_corrected = layer.bias_cache / \\\n",
    "\t\t\t(1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "\t\t# Vanilla SGD parameter update + normalization\n",
    "\t\t# with square rooted cache\n",
    "\t\tlayer.weights += -self.current_learning_rate * \\\n",
    "\t\t\t\t\t\t weight_momentums_corrected / \\\n",
    "\t\t\t\t\t\t (np.sqrt(weight_cache_corrected) +\n",
    "\t\t\t\t\t\t\t self.epsilon)\n",
    "\t\tlayer.biases += -self.current_learning_rate * \\\n",
    "\t\t\t\t\t\t bias_momentums_corrected / \\\n",
    "\t\t\t\t\t\t (np.sqrt(bias_cache_corrected) +\n",
    "\t\t\t\t\t\t\t self.epsilon)\n",
    "\n",
    "\t# Call once after any parameter updates\n",
    "\tdef post_update_params(self):\n",
    "\t\tself.iterations += 1\n",
    "\n",
    "\n",
    "\n",
    "# Common loss class\n",
    "class Loss:\n",
    "\n",
    "\t# Regularization loss calculation\n",
    "\tdef regularization_loss(self):\n",
    "\n",
    "\t\t# 0 by default\n",
    "\t\tregularization_loss = 0\n",
    "\n",
    "\t\t# Calculate regularization loss\n",
    "\t\t# iterate all trainable layers\n",
    "\t\tfor layer in self.trainable_layers:\n",
    "\n",
    "\t\t\t# L1 regularization - weights\n",
    "\t\t\t# calculate only when factor greater than 0\n",
    "\t\t\tif layer.weight_regularizer_l1 > 0:\n",
    "\t\t\t\tregularization_loss += layer.weight_regularizer_l1 * \\\n",
    "\t\t\t\t\t\t\t\t\t   np.sum(np.abs(layer.weights))\n",
    "\n",
    "\t\t\t# L2 regularization - weights\n",
    "\t\t\tif layer.weight_regularizer_l2 > 0:\n",
    "\t\t\t\tregularization_loss += layer.weight_regularizer_l2 * \\\n",
    "\t\t\t\t\t\t\t\t\t   np.sum(layer.weights * \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t  layer.weights)\n",
    "\n",
    "\t\t\t# L1 regularization - biases\n",
    "\t\t\t# calculate only when factor greater than 0\n",
    "\t\t\tif layer.bias_regularizer_l1 > 0:\n",
    "\t\t\t\tregularization_loss += layer.bias_regularizer_l1 * \\\n",
    "\t\t\t\t\t\t\t\t\t   np.sum(np.abs(layer.biases))\n",
    "\n",
    "\t\t\t# L2 regularization - biases\n",
    "\t\t\tif layer.bias_regularizer_l2 > 0:\n",
    "\t\t\t\tregularization_loss += layer.bias_regularizer_l2 * \\\n",
    "\t\t\t\t\t\t\t\t\t   np.sum(layer.biases * \\\n",
    "\t\t\t\t\t\t\t\t\t\t\t  layer.biases)\n",
    "\n",
    "\t\treturn regularization_loss\n",
    "\n",
    "\t# Set/remember trainable layers\n",
    "\tdef remember_trainable_layers(self, trainable_layers):\n",
    "\t\tself.trainable_layers = trainable_layers\n",
    "\n",
    "\n",
    "\t# Calculates the data and regularization losses\n",
    "\t# given model output and ground truth values\n",
    "\tdef calculate(self, output, y, *, include_regularization=False):\n",
    "\n",
    "\t\t# Calculate sample losses\n",
    "\t\tsample_losses = self.forward(output, y)\n",
    "\n",
    "\t\t# Calculate mean loss\n",
    "\t\tdata_loss = np.mean(sample_losses)\n",
    "\n",
    "\t\t# Add accumulated sum of losses and sample count\n",
    "\t\tself.accumulated_sum += np.sum(sample_losses)\n",
    "\t\tself.accumulated_count += len(sample_losses)\n",
    "\n",
    "\t\t# If just data loss - return it\n",
    "\t\tif not include_regularization:\n",
    "\t\t\treturn data_loss\n",
    "\n",
    "\t\t# Return the data and regularization losses\n",
    "\t\treturn data_loss, self.regularization_loss()\n",
    "\n",
    "\t# Calculates accumulated loss\n",
    "\tdef calculate_accumulated(self, *, include_regularization=False):\n",
    "\n",
    "\t\t# Calculate mean loss\n",
    "\t\tdata_loss = self.accumulated_sum / self.accumulated_count\n",
    "\n",
    "\t\t# If just data loss - return it\n",
    "\t\tif not include_regularization:\n",
    "\t\t\treturn data_loss\n",
    "\n",
    "\t\t# Return the data and regularization losses\n",
    "\t\treturn data_loss, self.regularization_loss()\n",
    "\n",
    "\t# Reset variables for accumulated loss\n",
    "\tdef new_pass(self):\n",
    "\t\tself.accumulated_sum = 0\n",
    "\t\tself.accumulated_count = 0\n",
    "\n",
    "\n",
    "\n",
    "# Mean Squared Error loss\n",
    "class Loss_MeanSquaredError(Loss):  # L2 loss\n",
    "\n",
    "\t# Forward pass\n",
    "\tdef forward(self, y_pred, y_true):\n",
    "\n",
    "\t\t# Calculate loss\n",
    "\t\tsample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
    "\n",
    "\t\t# Return losses\n",
    "\t\treturn sample_losses\n",
    "\n",
    "\t# Backward pass\n",
    "\tdef backward(self, dvalues, y_true):\n",
    "\n",
    "\t\t# Number of samples\n",
    "\t\tsamples = len(dvalues)\n",
    "\t\t# Number of outputs in every sample\n",
    "\t\t# We'll use the first sample to count them\n",
    "\t\toutputs = len(dvalues[0])\n",
    "\n",
    "\t\t# Gradient on values\n",
    "\t\tself.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "\t\t# Normalize gradient\n",
    "\t\tself.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Mean Absolute Error loss\n",
    "class Loss_MeanAbsoluteError(Loss):  # L1 loss\n",
    "\n",
    "\tdef forward(self, y_pred, y_true):\n",
    "\n",
    "\t\t# Calculate loss\n",
    "\t\tsample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
    "\n",
    "\t\t# Return losses\n",
    "\t\treturn sample_losses\n",
    "\n",
    "\n",
    "\t# Backward pass\n",
    "\tdef backward(self, dvalues, y_true):\n",
    "\n",
    "\t\t# Number of samples\n",
    "\t\tsamples = len(dvalues)\n",
    "\t\t# Number of outputs in every sample\n",
    "\t\t# We'll use the first sample to count them\n",
    "\t\toutputs = len(dvalues[0])\n",
    "\n",
    "\t\t# Calculate gradient\n",
    "\t\tself.dinputs = np.sign(y_true - dvalues) / outputs\n",
    "\t\t# Normalize gradient\n",
    "\t\tself.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# Common accuracy class\n",
    "class Accuracy:\n",
    "\n",
    "\t# Calculates an accuracy\n",
    "\t# given predictions and ground truth values\n",
    "\tdef calculate(self, predictions, y):\n",
    "\n",
    "\t\t# Get comparison results\n",
    "\t\tcomparisons = self.compare(predictions, y)\n",
    "\n",
    "\t\t# Calculate an accuracy\n",
    "\t\taccuracy = np.mean(comparisons)\n",
    "\n",
    "\t\t# Add accumulated sum of matching values and sample count\n",
    "\t\tself.accumulated_sum += np.sum(comparisons)\n",
    "\t\tself.accumulated_count += len(comparisons)\n",
    "\n",
    "\t\t# Return accuracy\n",
    "\t\treturn accuracy\n",
    "\n",
    "\t# Calculates accumulated accuracy\n",
    "\tdef calculate_accumulated(self):\n",
    "\n",
    "\t\t# Calculate an accuracy\n",
    "\t\taccuracy = self.accumulated_sum / self.accumulated_count\n",
    "\n",
    "\t\t# Return the data and regularization losses\n",
    "\t\treturn accuracy\n",
    "\n",
    "\t# Reset variables for accumulated accuracy\n",
    "\tdef new_pass(self):\n",
    "\t\tself.accumulated_sum = 0\n",
    "\t\tself.accumulated_count = 0\n",
    "\n",
    "\n",
    "\n",
    "# Accuracy calculation for classification model\n",
    "class Accuracy_Categorical(Accuracy):\n",
    "\n",
    "\tdef __init__(self, *, binary=False):\n",
    "\t\t# Binary mode?\n",
    "\t\tself.binary = binary\n",
    "\n",
    "\t# No initialization is needed\n",
    "\tdef init(self, y):\n",
    "\t\tpass\n",
    "\n",
    "\t# Compares predictions to the ground truth values\n",
    "\tdef compare(self, predictions, y):\n",
    "\t\tif not self.binary and len(y.shape) == 2:\n",
    "\t\t\ty = np.argmax(y, axis=1)\n",
    "\t\treturn predictions == y\n",
    "\n",
    "\n",
    "# Accuracy calculation for regression model\n",
    "class Accuracy_Regression(Accuracy):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\t# Create precision property\n",
    "\t\tself.precision = None\n",
    "\n",
    "\t# Calculates precision value\n",
    "\t# based on passed-in ground truth values\n",
    "\tdef init(self, y, reinit=False):\n",
    "\t\tif self.precision is None or reinit:\n",
    "\t\t\tself.precision = np.std(y) / 250\n",
    "\n",
    "\t# Compares predictions to the ground truth values\n",
    "\tdef compare(self, predictions, y):\n",
    "\t\treturn np.absolute(predictions - y) < self.precision\n",
    "\n",
    "\n",
    "# Model class\n",
    "class Model:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\t# Create a list of network objects\n",
    "\t\tself.layers = []\n",
    "\t\t# Softmax classifier's output object\n",
    "\t\tself.softmax_classifier_output = None\n",
    "\n",
    "\t# Add objects to the model\n",
    "\tdef add(self, layer):\n",
    "\t\tself.layers.append(layer)\n",
    "\n",
    "\n",
    "\t# Set loss, optimizer and accuracy\n",
    "\tdef set(self, *, loss=None, optimizer=None, accuracy=None):\n",
    "\n",
    "\t\tif loss is not None:\n",
    "\t\t\tself.loss = loss\n",
    "\n",
    "\t\tif optimizer is not None:\n",
    "\t\t\tself.optimizer = optimizer\n",
    "\n",
    "\t\tif accuracy is not None:\n",
    "\t\t\tself.accuracy = accuracy\n",
    "\n",
    "\t# Finalize the model\n",
    "\tdef finalize(self):\n",
    "\n",
    "\t\t# Create and set the input layer\n",
    "\t\tself.input_layer = Layer_Input()\n",
    "\n",
    "\t\t# Count all the objects\n",
    "\t\tlayer_count = len(self.layers)\n",
    "\n",
    "\t\t# Initialize a list containing trainable layers:\n",
    "\t\tself.trainable_layers = []\n",
    "\n",
    "\t\t# Iterate the objects\n",
    "\t\tfor i in range(layer_count):\n",
    "\n",
    "\t\t\t# If it's the first layer,\n",
    "\t\t\t# the previous layer object is the input layer\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tself.layers[i].prev = self.input_layer\n",
    "\t\t\t\tself.layers[i].next = self.layers[i+1]\n",
    "\n",
    "\t\t\t# All layers except for the first and the last\n",
    "\t\t\telif i < layer_count - 1:\n",
    "\t\t\t\tself.layers[i].prev = self.layers[i-1]\n",
    "\t\t\t\tself.layers[i].next = self.layers[i+1]\n",
    "\n",
    "\t\t\t# The last layer - the next object is the loss\n",
    "\t\t\t# Also let's save aside the reference to the last object\n",
    "\t\t\t# whose output is the model's output\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.layers[i].prev = self.layers[i-1]\n",
    "\t\t\t\tself.layers[i].next = self.loss\n",
    "\t\t\t\tself.output_layer_activation = self.layers[i]\n",
    "\n",
    "\n",
    "\t\t\t# If layer contains an attribute called \"weights\",\n",
    "\t\t\t# it's a trainable layer -\n",
    "\t\t\t# add it to the list of trainable layers\n",
    "\t\t\t# We don't need to check for biases -\n",
    "\t\t\t# checking for weights is enough\n",
    "\t\t\tif hasattr(self.layers[i], 'weights'):\n",
    "\t\t\t\tself.trainable_layers.append(self.layers[i])\n",
    "\n",
    "\t\t# Update loss object with trainable layers\n",
    "\t\tif self.loss is not None:\n",
    "\t\t\tprint(self.trainable_layers)\n",
    "\t\t\tself.loss.remember_trainable_layers(self.trainable_layers)\n",
    "\n",
    "\t\t# If output activation is Softmax and\n",
    "\t\t# loss function is Categorical Cross-Entropy\n",
    "\t\t# create an object of combined activation\n",
    "\t\t# and loss function containing\n",
    "\t\t# faster gradient calculation\n",
    "\n",
    "\t# Train the model\n",
    "\tdef train(self, X, y, *, epochs=1, batch_size=None,\n",
    "\t\t\t  print_every=1, validation_data=None):\n",
    "\n",
    "\t\t# Initialize accuracy object\n",
    "\t\tself.accuracy.init(y)\n",
    "\n",
    "\t\t# Default value if batch size is not being set\n",
    "\t\ttrain_steps = 1\n",
    "\n",
    "\t\t# Calculate number of steps\n",
    "\t\tif batch_size is not None:\n",
    "\t\t\ttrain_steps = len(X) // batch_size\n",
    "\t\t\t# Dividing rounds down. If there are some remaining\n",
    "\t\t\t# data but not a full batch, this won't include it\n",
    "\t\t\t# Add `1` to include this not full batch\n",
    "\t\t\tif train_steps * batch_size < len(X):\n",
    "\t\t\t\ttrain_steps += 1\n",
    "\n",
    "\n",
    "\t\t# Main training loop\n",
    "\t\tfor epoch in range(1, epochs+1):\n",
    "\n",
    "\t\t\t# Print epoch number\n",
    "\t\t\tprint(f'epoch: {epoch}')\n",
    "\n",
    "\t\t\t# Reset accumulated values in loss and accuracy objects\n",
    "\t\t\tself.loss.new_pass()\n",
    "\t\t\tself.accuracy.new_pass()\n",
    "\n",
    "\t\t\t# Iterate over steps\n",
    "\t\t\tfor step in range(train_steps):\n",
    "\n",
    "\t\t\t\t# If batch size is not set -\n",
    "\t\t\t\t# train using one step and full dataset\n",
    "\t\t\t\tif batch_size is None:\n",
    "\t\t\t\t\tbatch_X = X\n",
    "\t\t\t\t\tbatch_y = y\n",
    "\n",
    "\t\t\t\t# Otherwise slice a batch\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tbatch_X = X[step*batch_size:(step+1)*batch_size]\n",
    "\t\t\t\t\tbatch_y = y[step*batch_size:(step+1)*batch_size]\n",
    "\n",
    "\t\t\t\t# Perform the forward pass\n",
    "\t\t\t\toutput = self.forward(batch_X, training=True)\n",
    "\n",
    "\t\t\t\t# Calculate loss\n",
    "\t\t\t\tdata_loss, regularization_loss = \\\n",
    "\t\t\t\t\tself.loss.calculate(output, batch_y,\n",
    "\t\t\t\t\t\t\t\t\t\tinclude_regularization=True)\n",
    "\t\t\t\tloss = data_loss + regularization_loss\n",
    "\n",
    "\t\t\t\t# Get predictions and calculate an accuracy\n",
    "\t\t\t\tpredictions = self.output_layer_activation.predictions(\n",
    "\t\t\t\t\t\t\t\t  output)\n",
    "\t\t\t\taccuracy = self.accuracy.calculate(predictions,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   batch_y)\n",
    "\n",
    "\t\t\t\t# Perform backward pass\n",
    "\t\t\t\tself.backward(output, batch_y)\n",
    "\n",
    "\t\t\t\t# Optimize (update parameters)\n",
    "\t\t\t\tself.optimizer.pre_update_params()\n",
    "\t\t\t\tfor layer in self.trainable_layers:\n",
    "\t\t\t\t\tself.optimizer.update_params(layer)\n",
    "\t\t\t\tself.optimizer.post_update_params()\n",
    "\n",
    "\n",
    "\t\t\t\t# Print a summary\n",
    "\t\t\t\tif not step % print_every or step == train_steps - 1:\n",
    "\t\t\t\t\tprint(f'step: {step}, ' +\n",
    "\t\t\t\t\t\t  f'acc: {accuracy:.3f}, ' +\n",
    "\t\t\t\t\t\t  f'loss: {loss:.3f} (' +\n",
    "\t\t\t\t\t\t  f'data_loss: {data_loss:.3f}, ' +\n",
    "\t\t\t\t\t\t  f'reg_loss: {regularization_loss:.3f}), ' +\n",
    "\t\t\t\t\t\t  f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "\t\t\t# Get and print epoch loss and accuracy\n",
    "\t\t\tepoch_data_loss, epoch_regularization_loss = \\\n",
    "\t\t\t\tself.loss.calculate_accumulated(\n",
    "\t\t\t\t\tinclude_regularization=True)\n",
    "\t\t\tepoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "\t\t\tepoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "\t\t\tprint(f'training, ' +\n",
    "\t\t\t\t  f'acc: {epoch_accuracy:.3f}, ' +\n",
    "\t\t\t\t  f'loss: {epoch_loss:.3f} (' +\n",
    "\t\t\t\t  f'data_loss: {epoch_data_loss:.3f}, ' +\n",
    "\t\t\t\t  f'reg_loss: {epoch_regularization_loss:.3f}), ' +\n",
    "\t\t\t\t  f'lr: {self.optimizer.current_learning_rate}')\n",
    "\n",
    "\t\t\t# If there is the validation data\n",
    "\t\t\tif validation_data is not None:\n",
    "\n",
    "\t\t\t\t# Evaluate the model:\n",
    "\t\t\t\tself.evaluate(*validation_data,\n",
    "\t\t\t\t\t\t\t  batch_size=batch_size)\n",
    "\n",
    "\t# Evaluates the model using passed-in dataset\n",
    "\tdef evaluate(self, X_val, y_val, *, batch_size=None):\n",
    "\n",
    "\t\t# Default value if batch size is not being set\n",
    "\t\tvalidation_steps = 1\n",
    "\n",
    "\t\t# Calculate number of steps\n",
    "\t\tif batch_size is not None:\n",
    "\t\t\tvalidation_steps = len(X_val) // batch_size\n",
    "\t\t\t# Dividing rounds down. If there are some remaining\n",
    "\t\t\t# data but not a full batch, this won't include it\n",
    "\t\t\t# Add `1` to include this not full batch\n",
    "\t\t\tif validation_steps * batch_size < len(X_val):\n",
    "\t\t\t\tvalidation_steps += 1\n",
    "\n",
    "\t\t# Reset accumulated values in loss\n",
    "\t\t# and accuracy objects\n",
    "\t\tself.loss.new_pass()\n",
    "\t\tself.accuracy.new_pass()\n",
    "\n",
    "\n",
    "\t\t# Iterate over steps\n",
    "\t\tfor step in range(validation_steps):\n",
    "\n",
    "\t\t\t# If batch size is not set -\n",
    "\t\t\t# train using one step and full dataset\n",
    "\t\t\tif batch_size is None:\n",
    "\t\t\t\tbatch_X = X_val\n",
    "\t\t\t\tbatch_y = y_val\n",
    "\n",
    "\t\t\t# Otherwise slice a batch\n",
    "\t\t\telse:\n",
    "\t\t\t\tbatch_X = X_val[\n",
    "\t\t\t\t\tstep*batch_size:(step+1)*batch_size\n",
    "\t\t\t\t]\n",
    "\t\t\t\tbatch_y = y_val[\n",
    "\t\t\t\t\tstep*batch_size:(step+1)*batch_size\n",
    "\t\t\t\t]\n",
    "\n",
    "\t\t\t# Perform the forward pass\n",
    "\t\t\toutput = self.forward(batch_X, training=False)\n",
    "\n",
    "\t\t\t# Calculate the loss\n",
    "\t\t\tself.loss.calculate(output, batch_y)\n",
    "\n",
    "\t\t\t# Get predictions and calculate an accuracy\n",
    "\t\t\tpredictions = self.output_layer_activation.predictions(\n",
    "\t\t\t\t\t\t\t  output)\n",
    "\t\t\tself.accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "\t\t# Get and print validation loss and accuracy\n",
    "\t\tvalidation_loss = self.loss.calculate_accumulated()\n",
    "\t\tvalidation_accuracy = self.accuracy.calculate_accumulated()\n",
    "\n",
    "\t\t# Print a summary\n",
    "\t\tprint(f'validation, ' +\n",
    "\t\t\t  f'acc: {validation_accuracy:.3f}, ' +\n",
    "\t\t\t  f'loss: {validation_loss:.3f}')\n",
    "\n",
    "\t# Predicts on the samples\n",
    "\tdef predict(self, X, *, batch_size=None):\n",
    "\n",
    "\t\t# Default value if batch size is not being set\n",
    "\t\tprediction_steps = 1\n",
    "\n",
    "\t\t# Calculate number of steps\n",
    "\t\tif batch_size is not None:\n",
    "\t\t\tprediction_steps = len(X) // batch_size\n",
    "\n",
    "\t\t\t# Dividing rounds down. If there are some remaining\n",
    "\t\t\t# data but not a full batch, this won't include it\n",
    "\t\t\t# Add `1` to include this not full batch\n",
    "\t\t\tif prediction_steps * batch_size < len(X):\n",
    "\t\t\t\tprediction_steps += 1\n",
    "\n",
    "\t\t# Model outputs\n",
    "\t\toutput = []\n",
    "\n",
    "\t\t# Iterate over steps\n",
    "\t\tfor step in range(prediction_steps):\n",
    "\n",
    "\t\t\t# If batch size is not set -\n",
    "\t\t\t# train using one step and full dataset\n",
    "\t\t\tif batch_size is None:\n",
    "\t\t\t\tbatch_X = X\n",
    "\n",
    "\t\t\t# Otherwise slice a batch\n",
    "\t\t\telse:\n",
    "\t\t\t\tbatch_X = X[step*batch_size:(step+1)*batch_size]\n",
    "\n",
    "\t\t\t# Perform the forward pass\n",
    "\t\t\tbatch_output = self.forward(batch_X, training=False)\n",
    "\n",
    "\t\t\t# Append batch prediction to the list of predictions\n",
    "\t\t\toutput.append(batch_output)\n",
    "\n",
    "\t\t# Stack and return results\n",
    "\t\treturn np.vstack(output)\n",
    "\n",
    "\t# Performs forward pass\n",
    "\tdef forward(self, X, training):\n",
    "\n",
    "\t\t# Call forward method on the input layer\n",
    "\t\t# this will set the output property that\n",
    "\t\t# the first layer in \"prev\" object is expecting\n",
    "\t\tself.input_layer.forward(X, training)\n",
    "\n",
    "\t\t# Call forward method of every object in a chain\n",
    "\t\t# Pass output of the previous object as a parameter\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\tlayer.forward(layer.prev.output, training)\n",
    "\n",
    "\t\t# \"layer\" is now the last object from the list,\n",
    "\t\t# return its output\n",
    "\t\treturn layer.output\n",
    "\n",
    "\n",
    "\t# Performs backward pass\n",
    "\tdef backward(self, output, y):\n",
    "\n",
    "\t\t# If softmax classifier\n",
    "\t\tif self.softmax_classifier_output is not None:\n",
    "\t\t\t# First call backward method\n",
    "\t\t\t# on the combined activation/loss\n",
    "\t\t\t# this will set dinputs property\n",
    "\t\t\tself.softmax_classifier_output.backward(output, y)\n",
    "\n",
    "\t\t\t# Since we'll not call backward method of the last layer\n",
    "\t\t\t# which is Softmax activation\n",
    "\t\t\t# as we used combined activation/loss\n",
    "\t\t\t# object, let's set dinputs in this object\n",
    "\t\t\tself.layers[-1].dinputs = \\\n",
    "\t\t\t\tself.softmax_classifier_output.dinputs\n",
    "\n",
    "\t\t\t# Call backward method going through\n",
    "\t\t\t# all the objects but last\n",
    "\t\t\t# in reversed order passing dinputs as a parameter\n",
    "\t\t\tfor layer in reversed(self.layers[:-1]):\n",
    "\t\t\t\tlayer.backward(layer.next.dinputs)\n",
    "\n",
    "\t\t\treturn\n",
    "\n",
    "\t\t# First call backward method on the loss\n",
    "\t\t# this will set dinputs property that the last\n",
    "\t\t# layer will try to access shortly\n",
    "\t\tself.loss.backward(output, y)\n",
    "\n",
    "\t\t# Call backward method going through all the objects\n",
    "\t\t# in reversed order passing dinputs as a parameter\n",
    "\t\tfor layer in reversed(self.layers):\n",
    "\t\t\tlayer.backward(layer.next.dinputs)\n",
    "\n",
    "\t# Retrieves and returns parameters of trainable layers\n",
    "\tdef get_parameters(self):\n",
    "\n",
    "\t\t# Create a list for parameters\n",
    "\t\tparameters = []\n",
    "\n",
    "\t\t# Iterable trainable layers and get their parameters\n",
    "\t\tfor layer in self.trainable_layers:\n",
    "\t\t\tparameters.append(layer.get_parameters())\n",
    "\n",
    "\t\t# Return a list\n",
    "\t\treturn parameters\n",
    "\n",
    "\n",
    "\t# Updates the model with new parameters\n",
    "\tdef set_parameters(self, parameters):\n",
    "\n",
    "\t\t# Iterate over the parameters and layers\n",
    "\t\t# and update each layers with each set of the parameters\n",
    "\t\tfor parameter_set, layer in zip(parameters,\n",
    "\t\t\t\t\t\t\t\t\t\tself.trainable_layers):\n",
    "\t\t\tlayer.set_parameters(*parameter_set)\n",
    "\n",
    "\t# Saves the parameters to a file\n",
    "\tdef save_parameters(self, path):\n",
    "\n",
    "\t\t# Open a file in the binary-write mode\n",
    "\t\t# and save parameters into it\n",
    "\t\twith open(path, 'wb') as f:\n",
    "\t\t\tpickle.dump(self.get_parameters(), f)\n",
    "\n",
    "\t# Loads the weights and updates a model instance with them\n",
    "\tdef load_parameters(self, path):\n",
    "\n",
    "\t\t# Open file in the binary-read mode,\n",
    "\t\t# load weights and update trainable layers\n",
    "\t\twith open(path, 'rb') as f:\n",
    "\t\t\tself.set_parameters(pickle.load(f))\n",
    "\n",
    "\t# Saves the model\n",
    "\tdef save(self, path):\n",
    "\n",
    "\t\t# Make a deep copy of current model instance\n",
    "\t\tmodel = copy.deepcopy(self)\n",
    "\n",
    "\t\t# Reset accumulated values in loss and accuracy objects\n",
    "\t\tmodel.loss.new_pass()\n",
    "\t\tmodel.accuracy.new_pass()\n",
    "\n",
    "\t\t# Remove data from the input layer\n",
    "\t\t# and gradients from the loss object\n",
    "\t\tmodel.input_layer.__dict__.pop('output', None)\n",
    "\t\tmodel.loss.__dict__.pop('dinputs', None)\n",
    "\n",
    "\t\t# For each layer remove inputs, output and dinputs properties\n",
    "\t\tfor layer in model.layers:\n",
    "\t\t\tfor property in ['inputs', 'output', 'dinputs',\n",
    "\t\t\t\t\t\t\t 'dweights', 'dbiases']:\n",
    "\t\t\t\tlayer.__dict__.pop(property, None)\n",
    "\n",
    "\t\t# Open a file in the binary-write mode and save the model\n",
    "\t\twith open(path, 'wb') as f:\n",
    "\t\t\tpickle.dump(model, f)\n",
    "\n",
    "\n",
    "\t# Loads and returns a model\n",
    "\t@staticmethod\n",
    "\tdef load(path):\n",
    "\n",
    "\t\t# Open file in the binary-read mode, load a model\n",
    "\t\twith open(path, 'rb') as f:\n",
    "\t\t\tmodel = pickle.load(f)\n",
    "\n",
    "\t\t# Return a model\n",
    "\t\treturn model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input \"layer\"\n",
    "class Layer_Input:\n",
    "\n",
    "\t# Forward pass\n",
    "\tdef forward(self, inputs, training):\n",
    "\t\tself.output = inputs\n",
    "\n",
    "\tdef backward(self, dvalues):\n",
    "\t\tself.dinputs = dvalues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.Layer_Dense object at 0x7f35e82d9e50>, <__main__.Layer_Dense object at 0x7f35e82d9d00>]\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.add(Layer_Input())\n",
    "model.add(Layer_Dense(1, 256))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(256, 1))\n",
    "model.add(Activation_Softmax())\n",
    "\n",
    "model.set(loss=Loss_MeanAbsoluteError(),optimizer=Optimizer_Adam(), accuracy=Accuracy_Regression())\n",
    "model.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f3592ab59a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def train_function(x):\n",
    "    return x, np.sin(x)\n",
    "# define input sequence\n",
    "xaxis = np.arange(-1*np.pi, 1*np.pi, 0.01)\n",
    "ax_test = np.arange(8*np.pi, 10*np.pi, 0.01)\n",
    "X,y = train_function(xaxis)\n",
    "X_test,y_test = train_function(ax_test)\n",
    "X = np.array([X])\n",
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(629, 1)\n",
      "(629, 1)\n"
     ]
    }
   ],
   "source": [
    "X = np.reshape(X, (-1, 1))\n",
    "y = np.reshape(y, (-1, 1))\n",
    "X_test = np.reshape(X_test, (-1, 1))\n",
    "y_test = np.reshape(y_test, (-1, 1))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 2\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 3\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 4\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 5\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 6\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 7\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 8\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 9\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 10\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 11\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 12\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 13\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 14\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 15\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 16\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 17\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 18\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 19\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 20\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 21\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 22\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 23\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 24\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 25\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 26\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 27\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 28\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 29\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 30\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 31\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 32\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 33\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 34\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 35\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 36\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 37\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 38\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 39\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 40\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 41\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 42\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 43\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 44\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 45\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 46\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 47\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 48\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 49\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 50\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 51\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 52\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 53\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 54\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 55\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 56\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 57\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 58\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 59\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 60\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 61\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 62\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 63\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 64\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 65\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 66\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 67\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 68\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 69\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 70\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 71\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 72\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 73\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 74\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 75\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 76\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 77\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 78\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 79\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 80\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 81\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 82\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 83\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 84\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 85\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 86\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 87\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 88\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 89\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 90\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 91\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 92\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 93\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 94\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 95\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 96\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 97\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 98\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 99\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 100\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 101\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 102\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 103\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 104\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 105\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 106\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 107\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 108\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 109\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 110\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 111\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 112\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 113\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 114\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 115\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 116\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 117\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 118\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 119\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 120\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 121\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 122\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 123\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 124\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 125\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 126\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 127\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 128\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 129\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 130\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 131\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 132\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 133\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 134\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 135\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 136\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 137\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 138\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 139\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 140\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 141\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 142\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 143\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 144\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 145\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 146\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 147\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 148\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 149\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 150\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 151\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 152\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 153\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 154\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 155\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 156\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 157\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 158\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 159\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 160\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 161\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 162\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 163\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 164\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 165\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 166\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 167\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 168\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 169\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 170\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 171\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 172\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 173\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 174\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 175\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 176\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 177\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 178\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 179\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 180\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 181\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 182\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 183\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 184\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 185\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 186\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 187\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 188\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 189\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 190\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 191\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 192\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 193\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 194\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 195\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 196\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 197\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 198\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 199\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 200\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 201\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 202\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 203\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 204\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 205\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 206\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 207\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 208\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 209\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 210\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 211\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 212\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 213\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 214\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 215\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 216\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 217\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 218\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 219\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 220\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 221\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 222\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 223\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 224\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 225\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 226\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 227\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 228\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 229\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 230\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 231\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 232\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 233\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 234\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 235\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 236\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 237\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 238\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 239\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 240\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 241\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 242\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 243\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 244\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 245\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 246\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 247\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 248\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 249\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 250\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 251\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 252\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 253\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 254\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 255\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 256\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 257\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 258\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 259\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 260\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 261\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 262\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 263\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 264\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 265\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 266\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 267\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 268\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 269\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 270\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 271\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 272\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 273\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 274\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 275\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 276\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 277\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 278\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 279\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 280\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 281\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 282\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 283\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 284\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 285\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 286\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 287\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 288\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 289\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 290\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 291\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 292\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 293\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 294\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 295\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 296\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 297\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 298\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 299\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 300\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 301\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 302\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 303\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 304\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 305\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 306\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 307\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 308\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 309\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 310\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 311\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 312\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 313\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 314\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 315\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 316\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 317\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 318\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 319\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 320\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 321\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 322\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 323\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 324\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 325\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 326\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 327\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 328\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 329\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 330\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 331\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 332\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 333\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 334\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 335\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 336\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 337\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 338\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 339\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 340\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 341\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 342\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 343\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 344\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 345\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 346\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 347\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 348\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 349\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 350\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 351\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 352\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 353\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 354\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 355\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 356\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 357\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 358\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 359\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 360\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 361\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 362\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 363\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 364\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 365\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 366\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 367\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 368\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 369\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 370\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 371\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 372\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 373\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 374\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 375\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 376\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 377\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 378\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 379\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 380\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 381\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 382\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 383\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 384\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 385\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 386\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 387\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 388\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 389\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 390\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 391\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 392\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 393\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 394\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 395\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 396\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 397\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 398\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 399\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 400\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 401\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 402\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 403\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 404\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 405\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 406\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 407\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 408\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 409\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 410\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 411\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 412\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 413\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 414\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 415\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 416\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 417\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 418\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 419\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 420\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 421\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 422\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 423\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 424\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 425\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 426\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 427\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 428\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 429\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 430\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 431\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 432\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 433\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 434\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 435\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 436\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 437\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 438\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 439\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 440\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 441\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 442\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 443\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 444\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 445\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 446\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 447\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 448\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 449\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 450\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 451\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 452\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 453\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 454\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 455\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 456\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 457\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 458\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 459\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 460\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 461\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 462\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 463\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 464\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 465\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 466\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 467\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 468\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 469\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 470\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 471\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 472\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 473\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 474\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 475\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 476\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 477\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 478\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 479\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 480\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 481\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 482\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 483\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 484\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 485\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 486\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 487\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 488\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 489\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 490\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 491\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 492\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 493\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 494\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 495\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 496\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 497\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 498\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 499\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "epoch: 500\n",
      "step: 0, acc: 0.003, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n",
      "training, acc: 2.000, loss: 1.000 (data_loss: 1.000, reg_loss: 0.000), lr: 0.001\n"
     ]
    }
   ],
   "source": [
    "model.train(X, y, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f358a9b7b50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f71568b777833ccf7e25d0140e2de381dba3fc5405f5c7c60d120ab83c46892e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
